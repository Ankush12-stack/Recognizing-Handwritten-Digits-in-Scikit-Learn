{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the handwritten digit datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# digit contain the dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# dir function use to display the attributes of the dataset\n",
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df5ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# outputting the picture value as a series of numbers\n",
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e93cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the matplotlib libraries pyplot function\n",
    "import matplotlib.pyplot as plt\n",
    "# defining the function plot_multi\n",
    " \n",
    "def plot_multi(i):\n",
    "    nplots = 16\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    for j in range(nplots):\n",
    "        plt.subplot(4, 4, j+1)\n",
    "        plt.imshow(digits.images[i+j], cmap='binary')\n",
    "        plt.title(digits.target[i+j])\n",
    "        plt.axis('off')\n",
    "    # printing the each digits in the dataset.\n",
    "    plt.show()\n",
    " \n",
    "    plot_multi(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a019954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the 2 dimensional array to one dimensional array\n",
    "y = digits.target\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    " \n",
    "# gives the  shape of the data\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e23491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the one-dimensional array's values\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97705d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very first 1000 photographs and\n",
    "# labels will be used in training.\n",
    "x_train = x[:1000]\n",
    "y_train = y[:1000]\n",
    "\n",
    "# The leftover dataset will be utilised to\n",
    "# test the network's performance later on.\n",
    "x_test = x[1000:]\n",
    "y_test = y[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab1444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the MLP classifier from sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# calling the MLP classifier with specific parameters\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,),\n",
    "\t\t\t\t\tactivation='logistic',\n",
    "\t\t\t\t\talpha=1e-4, solver='sgd',\n",
    "\t\t\t\t\ttol=1e-4, random_state=1,\n",
    "\t\t\t\t\tlearning_rate_init=.1,\n",
    "\t\t\t\t\tverbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b711480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.22958289\n",
      "Iteration 2, loss = 1.91207743\n",
      "Iteration 3, loss = 1.62507727\n",
      "Iteration 4, loss = 1.32649842\n",
      "Iteration 5, loss = 1.06100535\n",
      "Iteration 6, loss = 0.83995513\n",
      "Iteration 7, loss = 0.67806075\n",
      "Iteration 8, loss = 0.55175832\n",
      "Iteration 9, loss = 0.45840445\n",
      "Iteration 10, loss = 0.39149735\n",
      "Iteration 11, loss = 0.33676351\n",
      "Iteration 12, loss = 0.29059880\n",
      "Iteration 13, loss = 0.25437208\n",
      "Iteration 14, loss = 0.22838372\n",
      "Iteration 15, loss = 0.20200554\n",
      "Iteration 16, loss = 0.18186565\n",
      "Iteration 17, loss = 0.16461183\n",
      "Iteration 18, loss = 0.14990228\n",
      "Iteration 19, loss = 0.13892154\n",
      "Iteration 20, loss = 0.12833784\n",
      "Iteration 21, loss = 0.12138920\n",
      "Iteration 22, loss = 0.11407971\n",
      "Iteration 23, loss = 0.10677664\n",
      "Iteration 24, loss = 0.10037149\n",
      "Iteration 25, loss = 0.09593187\n",
      "Iteration 26, loss = 0.09250135\n",
      "Iteration 27, loss = 0.08676698\n",
      "Iteration 28, loss = 0.08356043\n",
      "Iteration 29, loss = 0.08209789\n",
      "Iteration 30, loss = 0.07649168\n",
      "Iteration 31, loss = 0.07410898\n",
      "Iteration 32, loss = 0.07126869\n",
      "Iteration 33, loss = 0.06926956\n",
      "Iteration 34, loss = 0.06578496\n",
      "Iteration 35, loss = 0.06374913\n",
      "Iteration 36, loss = 0.06175492\n",
      "Iteration 37, loss = 0.05975664\n",
      "Iteration 38, loss = 0.05764485\n",
      "Iteration 39, loss = 0.05623663\n",
      "Iteration 40, loss = 0.05420966\n",
      "Iteration 41, loss = 0.05413911\n",
      "Iteration 42, loss = 0.05256140\n",
      "Iteration 43, loss = 0.05020265\n",
      "Iteration 44, loss = 0.04902779\n",
      "Iteration 45, loss = 0.04788382\n",
      "Iteration 46, loss = 0.04655532\n",
      "Iteration 47, loss = 0.04586089\n",
      "Iteration 48, loss = 0.04451758\n",
      "Iteration 49, loss = 0.04341598\n",
      "Iteration 50, loss = 0.04238096\n",
      "Iteration 51, loss = 0.04162200\n",
      "Iteration 52, loss = 0.04076839\n",
      "Iteration 53, loss = 0.04003180\n",
      "Iteration 54, loss = 0.03907774\n",
      "Iteration 55, loss = 0.03815565\n",
      "Iteration 56, loss = 0.03791975\n",
      "Iteration 57, loss = 0.03706276\n",
      "Iteration 58, loss = 0.03617874\n",
      "Iteration 59, loss = 0.03593227\n",
      "Iteration 60, loss = 0.03504175\n",
      "Iteration 61, loss = 0.03441259\n",
      "Iteration 62, loss = 0.03397449\n",
      "Iteration 63, loss = 0.03326990\n",
      "Iteration 64, loss = 0.03305025\n",
      "Iteration 65, loss = 0.03244893\n",
      "Iteration 66, loss = 0.03191504\n",
      "Iteration 67, loss = 0.03132169\n",
      "Iteration 68, loss = 0.03079707\n",
      "Iteration 69, loss = 0.03044946\n",
      "Iteration 70, loss = 0.03005546\n",
      "Iteration 71, loss = 0.02960555\n",
      "Iteration 72, loss = 0.02912799\n",
      "Iteration 73, loss = 0.02859103\n",
      "Iteration 74, loss = 0.02825959\n",
      "Iteration 75, loss = 0.02788968\n",
      "Iteration 76, loss = 0.02748725\n",
      "Iteration 77, loss = 0.02721247\n",
      "Iteration 78, loss = 0.02686225\n",
      "Iteration 79, loss = 0.02635636\n",
      "Iteration 80, loss = 0.02607439\n",
      "Iteration 81, loss = 0.02577613\n",
      "Iteration 82, loss = 0.02553642\n",
      "Iteration 83, loss = 0.02518749\n",
      "Iteration 84, loss = 0.02484300\n",
      "Iteration 85, loss = 0.02455379\n",
      "Iteration 86, loss = 0.02432480\n",
      "Iteration 87, loss = 0.02398548\n",
      "Iteration 88, loss = 0.02376004\n",
      "Iteration 89, loss = 0.02341261\n",
      "Iteration 90, loss = 0.02318255\n",
      "Iteration 91, loss = 0.02296065\n",
      "Iteration 92, loss = 0.02274048\n",
      "Iteration 93, loss = 0.02241054\n",
      "Iteration 94, loss = 0.02208181\n",
      "Iteration 95, loss = 0.02190861\n",
      "Iteration 96, loss = 0.02174404\n",
      "Iteration 97, loss = 0.02156939\n",
      "Iteration 98, loss = 0.02119768\n",
      "Iteration 99, loss = 0.02101874\n",
      "Iteration 100, loss = 0.02078230\n",
      "Iteration 101, loss = 0.02061573\n",
      "Iteration 102, loss = 0.02039802\n",
      "Iteration 103, loss = 0.02017245\n",
      "Iteration 104, loss = 0.01997162\n",
      "Iteration 105, loss = 0.01989280\n",
      "Iteration 106, loss = 0.01963828\n",
      "Iteration 107, loss = 0.01941850\n",
      "Iteration 108, loss = 0.01933154\n",
      "Iteration 109, loss = 0.01911473\n",
      "Iteration 110, loss = 0.01905371\n",
      "Iteration 111, loss = 0.01876085\n",
      "Iteration 112, loss = 0.01860656\n",
      "Iteration 113, loss = 0.01848655\n",
      "Iteration 114, loss = 0.01834844\n",
      "Iteration 115, loss = 0.01818981\n",
      "Iteration 116, loss = 0.01798523\n",
      "Iteration 117, loss = 0.01783630\n",
      "Iteration 118, loss = 0.01771441\n",
      "Iteration 119, loss = 0.01749814\n",
      "Iteration 120, loss = 0.01738339\n",
      "Iteration 121, loss = 0.01726549\n",
      "Iteration 122, loss = 0.01709638\n",
      "Iteration 123, loss = 0.01698340\n",
      "Iteration 124, loss = 0.01684606\n",
      "Iteration 125, loss = 0.01667016\n",
      "Iteration 126, loss = 0.01654172\n",
      "Iteration 127, loss = 0.01641832\n",
      "Iteration 128, loss = 0.01630111\n",
      "Iteration 129, loss = 0.01623051\n",
      "Iteration 130, loss = 0.01612736\n",
      "Iteration 131, loss = 0.01590220\n",
      "Iteration 132, loss = 0.01582485\n",
      "Iteration 133, loss = 0.01571372\n",
      "Iteration 134, loss = 0.01560349\n",
      "Iteration 135, loss = 0.01557688\n",
      "Iteration 136, loss = 0.01534420\n",
      "Iteration 137, loss = 0.01527883\n",
      "Iteration 138, loss = 0.01517545\n",
      "Iteration 139, loss = 0.01503663\n",
      "Iteration 140, loss = 0.01501192\n",
      "Iteration 141, loss = 0.01482535\n",
      "Iteration 142, loss = 0.01471388\n",
      "Iteration 143, loss = 0.01463948\n",
      "Iteration 144, loss = 0.01454059\n",
      "Iteration 145, loss = 0.01441742\n",
      "Iteration 146, loss = 0.01431741\n",
      "Iteration 147, loss = 0.01428414\n",
      "Iteration 148, loss = 0.01416364\n",
      "Iteration 149, loss = 0.01406742\n",
      "Iteration 150, loss = 0.01402651\n",
      "Iteration 151, loss = 0.01389720\n",
      "Iteration 152, loss = 0.01381412\n",
      "Iteration 153, loss = 0.01371300\n",
      "Iteration 154, loss = 0.01362465\n",
      "Iteration 155, loss = 0.01357048\n",
      "Iteration 156, loss = 0.01348760\n",
      "Iteration 157, loss = 0.01339543\n",
      "Iteration 158, loss = 0.01331941\n",
      "Iteration 159, loss = 0.01320812\n",
      "Iteration 160, loss = 0.01315415\n",
      "Iteration 161, loss = 0.01308279\n",
      "Iteration 162, loss = 0.01302708\n",
      "Iteration 163, loss = 0.01290042\n",
      "Iteration 164, loss = 0.01289267\n",
      "Iteration 165, loss = 0.01277558\n",
      "Iteration 166, loss = 0.01277238\n",
      "Iteration 167, loss = 0.01261308\n",
      "Iteration 168, loss = 0.01260611\n",
      "Iteration 169, loss = 0.01248789\n",
      "Iteration 170, loss = 0.01239662\n",
      "Iteration 171, loss = 0.01231743\n",
      "Iteration 172, loss = 0.01227346\n",
      "Iteration 173, loss = 0.01223136\n",
      "Iteration 174, loss = 0.01217211\n",
      "Iteration 175, loss = 0.01208682\n",
      "Iteration 176, loss = 0.01204707\n",
      "Iteration 177, loss = 0.01200225\n",
      "Iteration 178, loss = 0.01188677\n",
      "Iteration 179, loss = 0.01184993\n",
      "Iteration 180, loss = 0.01175130\n",
      "Iteration 181, loss = 0.01171178\n",
      "Iteration 182, loss = 0.01166052\n",
      "Iteration 183, loss = 0.01163843\n",
      "Iteration 184, loss = 0.01154892\n",
      "Iteration 185, loss = 0.01147629\n",
      "Iteration 186, loss = 0.01142365\n",
      "Iteration 187, loss = 0.01136608\n",
      "Iteration 188, loss = 0.01128053\n",
      "Iteration 189, loss = 0.01128869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(15,),\n",
       "              learning_rate_init=0.1, random_state=1, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e7d9607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 1)\n",
    "axes.plot(mlp.loss_curve_, 'o-')\n",
    "axes.set_xlabel(\"number of iteration\")\n",
    "axes.set_ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc082ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 3, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 5, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(x_test)\n",
    "predictions[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f6cee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 9, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "601ccadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9146800501882058"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the accuracy_score from the sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# calculating the accuracy with y_test and predictions\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bda11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
